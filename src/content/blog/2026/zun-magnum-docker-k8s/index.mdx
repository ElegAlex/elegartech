---
title: "Zun et Magnum face √† Docker/K8s : le d√©tour vaut-il vraiment le coup ?"
description: "Analyse comparative des services containers OpenStack (Zun/Magnum) face √† Docker et Kubernetes. Verdicts terrain et framework de d√©cision pour architectes."
date: 2026-01-31
categories: ["Syst√®mes", "Analyses"]
tags: ["OpenStack", "Kubernetes", "Docker", "Conteneurs", "Infrastructure"]
author: admin
draft: false
featured: false
---

**Verdict direct : Non, sauf si vous avez d√©j√† OpenStack.** Les services containers d'OpenStack (Zun et Magnum) apportent une multi-tenancy native et une int√©gration infrastructure remarquable, mais leur complexit√©, leurs communaut√©s r√©duites, et le co√ªt d'entr√©e d'une stack OpenStack compl√®te les rendent injustifiables pour quiconque n'exploite pas d√©j√† OpenStack pour ses VMs. Pour 95% des cas d'usage, K8s vanilla, k3s ou Nomad seront plus pragmatiques.

Ce constat repose sur l'analyse des t√©moignages terrain, de l'√©tat r√©el des projets en 2025, et des seuils de rentabilit√© quantifi√©s. Cet article d√©cortique les arbitrages techniques pour aider un architecte √† trancher entre ces deux mondes.

---

## Ce que Zun apporte r√©ellement (et ses limites critiques)

Zun, lanc√© en 2016, propose un mod√®le **Containers-as-a-Service** comparable √† AWS Fargate : ex√©cuter des conteneurs sans g√©rer de cluster Kubernetes. Les conteneurs deviennent des ressources OpenStack natives, b√©n√©ficiant automatiquement de l'authentification Keystone, du r√©seau Neutron, et des volumes Cinder.

L'int√©gration Neutron/Cinder repr√©sente le principal argument technique. Via **Kuryr-libnetwork**, chaque conteneur obtient une IP du pool Neutron avec security groups, floating IPs et isolation VXLAN/VLAN enterprise ‚Äî impossible √† r√©pliquer simplement avec Docker standalone. Les volumes Cinder apportent persistance r√©pliqu√©e, snapshots et chiffrement natif, l√† o√π Docker se limite aux volumes locaux.

|Fonctionnalit√©|Docker standalone|Zun + OpenStack|
|---|---|---|
|R√©seaux multi-tenant|Bridge partag√©|Isolation native Neutron|
|Volumes persistants|Filesystem local|Cinder avec r√©plication|
|Authentification|Aucune centralis√©e|Keystone RBAC|
|Quotas ressources|Non|Par projet OpenStack|

**Cependant, l'√©tat de la communaut√© Zun pose un probl√®me majeur.** Le projet compte **3 042 commits** et **101 contributeurs** au total, mais l'activit√© r√©cente se limite √† **10-20 commits par an** avec seulement **1 √† 5 contributeurs actifs**. Les releases suivent le cycle OpenStack (tous les 6 mois), mais les √©volutions sont minimales : corrections de compatibilit√© Python 3.12, fixes pour les nouvelles versions Docker. Aucune nouvelle fonctionnalit√© majeure depuis 2020.

La contrainte technique la plus handicapante : **Kuryr-libnetwork ne supporte pas Docker 23+** suite √† la suppression de l'option `--cluster-store`. Toute installation Zun n√©cessite de _pinner_ Docker en version 20.x, rendant impossible l'utilisation des fonctionnalit√©s r√©centes.

**Zun fait sens si** vous avez d√©j√† OpenStack et souhaitez exposer des conteneurs comme ressources cloud natives, avec multi-tenancy stricte et int√©gration Heat pour orchestrer VMs et conteneurs ensemble. **Zun est overkill** pour du d√©veloppement, du CI/CD, ou toute organisation sans OpenStack existant ‚Äî un d√©ploiement Zun minimal n√©cessite **6 services OpenStack** (Keystone, Neutron, MariaDB, RabbitMQ, Memcached, Kuryr, etcd) contre **5 minutes** pour installer Docker.

---

## Magnum vs Kubernetes auto-h√©berg√© : ce qu'on gagne vraiment

Magnum n'est pas une distribution Kubernetes modifi√©e mais une **couche de provisioning automatis√©** utilisant Heat pour d√©ployer des clusters K8s complets sur l'infrastructure OpenStack. Chaque cluster cr√©√© obtient automatiquement son r√©seau priv√© Neutron, ses load balancers Octavia pour l'API multi-master, et l'int√©gration cloud-provider-openstack pour les PersistentVolumes Cinder.

### La multi-tenancy : avantage d√©cisif ou surengineering ?

Kubernetes est **mono-tenant par design**. Impl√©menter une isolation s√©rieuse en vanilla K8s exige de configurer manuellement RBAC granulaire, NetworkPolicies, ResourceQuotas, PodSecurityStandards, voire des outils comme Loft ou vCluster. Magnum court-circuite tout cela : chaque cluster K8s obtient son **propre r√©seau priv√© Neutron** avec isolation au niveau hyperviseur. Les conteneurs d'un tenant ne peuvent physiquement pas communiquer avec ceux d'un autre.

|Crit√®re|Magnum|K8s vanilla|
|---|---|---|
|Isolation r√©seau|Native (Neutron)|NetworkPolicies √† configurer|
|Isolation compute|VMs s√©par√©es|Pods partagent le kernel|
|Configuration|Automatique|100+ objets √† cr√©er|
|Temps mise en ≈ìuvre|Minutes|Jours/semaines|

Cette multi-tenancy "hardware-level" justifie √† elle seule Magnum pour un cloud provider interne servant des √©quipes cloisonn√©es.

### Le retard de versions : un vrai frein

La version Dalmatian (octobre 2024) de Magnum embarque Kubernetes **v1.28.9**, alors que l'upstream atteint v1.31-1.32. Le **retard de 2 √† 4 versions** (~6-12 mois) provient de la d√©pendance aux images Hyperkube (officielles discontinu√©es apr√®s v1.18, n√©cessitant docker.io/rancher/) et aux tests d'int√©gration avec cloud-provider-openstack.

Le param√®tre `kube_tag` permet techniquement d'utiliser des versions plus r√©centes, mais **sans garantie de fonctionnement** ni support officiel.

---

## Le cas CERN : pourquoi √ßa marche pour eux

Le CERN exploite Magnum sur une infrastructure de **300 000 c≈ìurs CPU**, **9 000 hyperviseurs**, et **plus de 400 clusters Kubernetes** en production. Leur choix s'explique par trois facteurs impossibles √† r√©pliquer ailleurs :

**L'agnosticisme COE initial.** En 2016, le CERN devait supporter Docker Swarm, Kubernetes ET Mesos selon les pr√©f√©rences des √©quipes de physiciens. Magnum permettait ce choix sans multiplier les outils de provisioning.

**L'√©chelle et la multi-tenancy.** Avec 13 000 physiciens comme utilisateurs potentiels, l'isolation par projet Keystone √©tait indispensable. Chaque groupe peut cr√©er ses clusters K8s en self-service sans comprendre l'infrastructure sous-jacente.

**L'int√©gration stockage HEP.** Les workflows de physique des particules n√©cessitent l'acc√®s √† EOS (syst√®me de fichiers distribu√© CERN) et CephFS via Manila ‚Äî des int√©grations d√©velopp√©es en interne et contribu√©es upstream.

_"We had groups of people who were pushing for Kubernetes; we had groups of people who were already using Mesos; and others who were just using plain Docker"_ ‚Äî **Ricardo Rocha, CERN**

**Le CERN dispose aussi d'une √©quipe d√©di√©e** incluant Spyros Trigazis (PTL Magnum depuis Pike) et Belmiro Moreira (10+ ans d'architecture OpenStack), avec plus de 1 000 commits OpenStack depuis 2011. Cette expertise n'existe pas dans une organisation typique.

---

## T√©moignages terrain : la r√©alit√© op√©rationnelle

Les retours d'exp√©rience collect√©s sur Reddit, HackerNews et blogs techniques convergent vers un constat peu flatteur pour Magnum et Zun.

**Gcore (Head of PaaS Services, Andrei Novoselov)**, qui op√®re Magnum en production : _"It's as simple as riding a bike to operate Magnum in production, but it's like we're on fire, and everything around us is too, and we are in hell. If RabbitMQ goes down, Heat might not be able to handle it. The Heat stacks would crash, and there's no CLI way to get them up and running again."_ Leur temps de spin-up cluster : **20 minutes en moyenne**, avec une fragilit√© notable lors des incidents infrastructure.

**Un op√©rateur de 3 500+ VMs (jetbalsa, HackerNews)** : _"What people mean when it's 'consulting-ware' is that there are no real documentation on the failure states, tunables, and intermixed services. [...] An 8 Node support contract from RedHat was north of 80k/yr on top of telling me that Openstack is dead, have you tried Openshift."_

**JD.com (Fortune 500)** a migr√© int√©gralement d'OpenStack+Docker (JDOS 1.0) vers Kubernetes pur (JDOS 2.0), t√©moignant d'une tendance g√©n√©rale o√π les grandes organisations abandonnent les services containers OpenStack au profit de K8s vanilla d√©ploy√© sur l'infrastructure VM.

√Ä l'inverse, **les t√©moignages positifs viennent d'organisations avec OpenStack d√©j√† mature** : _"Once up and running, however, it is awesome"_ (BirAdam). Le point commun : une √©quipe experte et un investissement long terme.

---

## Framework de d√©cision : les seuils quantifi√©s

### Co√ªts op√©rationnels OpenStack

|M√©trique|Valeur|
|---|---|
|√âquipe minimum viable|**2 FTE** (couverture 24/7)|
|Ratio admin/serveurs|1 admin pour ~50 serveurs|
|Support Canonical avanc√©|**$1 500/serveur/an**|
|Consulting d√©ploiement|**$75 000 - $150 000**|
|Seuil rentabilit√© vs cloud public|**>400 VMs**|

### Disponibilit√© des comp√©tences

Le march√© de l'emploi refl√®te la r√©alit√© : Kubernetes appara√Æt dans **100% des offres DevOps/Cloud**, contre **8% pour OpenStack**. Recruter un expert OpenStack co√ªte un **premium de 15-25%** par rapport √† un profil K8s √©quivalent, avec un pool de candidats bien plus restreint.

### Arbres de d√©cision par sc√©nario

**PME/Startup (5-20 serveurs)** : ‚õî OpenStack = overkill absolu. Les **2 FTE minimum** et le **consulting √† $75K+** sont injustifiables. Solutions recommand√©es : k3s (production l√©g√®re), Docker Compose + Podman (applications simples), ou managed K8s si budget cloud disponible.

**Enterprise avec OpenStack existant** : ‚úÖ Magnum fait sens pour le provisioning K8s self-service avec synergies Keystone/Neutron/Cinder. Alternative viable : d√©ployer K8s via Terraform sur VMs Nova, √©vitant la couche Magnum/Heat mais perdant l'automatisation.

**Greenfield enterprise sans OpenStack** : üîÑ Recommander OpenStack+Magnum uniquement si : infrastructure >400 VMs pr√©vue, workloads mixtes VMs + containers obligatoires, souverainet√© donn√©es absolue, √©quipe ops de 3-5 FTE, et horizon >5 ans. Sinon, Rancher, OpenShift ou managed K8s seront plus pragmatiques.

---

## Les alternatives qui simplifient l'√©quation

**k3s** repr√©sente l'antith√®se de Magnum : binaire unique <70MB, installation en une commande, 512MB RAM minimum. Certifi√© CNCF, il offre une compatibilit√© totale avec l'√©cosyst√®me Kubernetes sans la complexit√©. Parfait pour PME, edge, IoT.

**HashiCorp Nomad** occupe une niche int√©ressante : orchestration multi-workload (containers, VMs, jobs batch, applications Java) via un binaire unique. Cloudflare l'utilise pour son edge computing. Si vous √™tes d√©j√† dans l'√©cosyst√®me HashiCorp (Terraform, Vault, Consul), Nomad s'int√®gre naturellement.

**Docker Swarm** reste maintenu par Mirantis avec engagement de support 3+ ans. Clients notables : MetLife, Royal Bank of Canada, S&P Global. Adapt√© aux √©quipes Docker existantes refusant la courbe d'apprentissage K8s, mais l'innovation a cess√©.

**Podman + systemd** (via Quadlets depuis Podman 4.4) permet une orchestration l√©g√®re sans d√©mon : fichiers unit systemd d√©claratifs, mode rootless natif, int√©gration journalctl. Id√©al pour machines uniques ou environnements edge sans K8s.

---

## Kata Containers : avec ou sans OpenStack ?

Kata Containers fonctionne **totalement ind√©pendamment d'OpenStack**. Ce runtime OCI cr√©ant des micro-VMs isol√©es s'int√®gre avec Docker, containerd, CRI-O et Kubernetes via CRI standard. Son h√©bergement par l'Open Infrastructure Foundation (ex-OpenStack Foundation) n'implique aucune d√©pendance technique.

|Aspect|Kata + OpenStack|Kata + K8s vanilla|
|---|---|---|
|Complexit√©|√âlev√©e (stack OS enti√®re)|Mod√©r√©e|
|Networking|Neutron-managed|CNI plugins|
|Adoption|T√©l√©coms, grandes entreprises|Hyperscalers, startups|

L'int√©gration OpenStack apporte le r√©seau Neutron et les volumes Cinder pour les containers Kata, mais **la majorit√© des d√©ploiements Kata se font sur K8s vanilla** ‚Äî Baidu, Ant Group, OVHcloud n'utilisent pas OpenStack pour leurs workloads Kata.

---

## L'ironie Kolla : quand Kubernetes orchestre OpenStack

**Kolla-Ansible** d√©ploie tous les services OpenStack dans des conteneurs Docker, transformant l'installation en playbooks Ansible reproductibles. Mais le projet **OpenStack-Helm** va plus loin : d√©ployer OpenStack _sur_ Kubernetes via Helm charts.

```
Sc√©nario traditionnel :
  Bare-Metal ‚Üí OpenStack ‚Üí VMs ‚Üí K8s ‚Üí Applications

Sc√©nario OpenStack-Helm :
  Bare-Metal ‚Üí Kubernetes ‚Üí OpenStack (pods K8s) ‚Üí VMs ‚Üí ...K8s nested ?
```

La question "qui orchestre qui ?" devient philosophique. **Airship**, utilis√© par AT&T, pousse cette logique : Kubernetes orchestre le lifecycle complet d'OpenStack, incluant le bare-metal provisioning. L'ironie ultime : OpenStack devient un workload Kubernetes comme un autre.

Cette architecture fait sens pour les op√©rateurs t√©l√©com souhaitant unifier leur tooling autour des patterns cloud-native (GitOps, op√©rateurs K8s, HPA) tout en conservant l'IaaS OpenStack pour les workloads legacy.

---

## Conclusion : les verdicts pragmatiques

**"Si tu n'as pas OpenStack, ne l'installe pas juste pour Zun/Magnum"** se confirme comme le conseil le plus sage. Le co√ªt d'entr√©e (√©quipe, consulting, maintenance) d√©passe syst√©matiquement les b√©n√©fices pour une organisation d√©marrant de z√©ro avec uniquement des besoins containers.

**"Si tu as d√©j√† OpenStack, Magnum peut simplifier le provisioning K8s"** reste vrai, avec la nuance que d√©ployer K8s via Terraform sur VMs Nova √©vite la couche Heat/Magnum tout en conservant les synergies r√©seau/stockage.

**"Zun = projet de niche avec communaut√© quasi-morte, √† √©viter pour du neuf"** se v√©rifie : 10-20 commits/an, incompatibilit√© Docker 23+, aucune adoption visible en production r√©cente.

**Le vrai arbitrage** pour un architecte en 2026 : acceptez que Kubernetes a gagn√© la guerre de l'orchestration containers. OpenStack reste pertinent comme IaaS pour VMs et bare-metal, mais ses services containers (Zun, Magnum) ciblent un segment de march√© tr√®s sp√©cifique ‚Äî cloud providers internes, t√©l√©coms, institutions de recherche avec √©quipes OpenStack matures et besoins de multi-tenancy hardware-level. Pour tous les autres cas, k3s, Rancher, Nomad ou managed K8s offriront un meilleur retour sur investissement.
